{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exam_LangDetector.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Классификация языков"
      ],
      "metadata": {
        "id": "yf8coVZE-qG6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SbZF3gcg2b36"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/exam/dutch.txt\", 'r') as infile:\n",
        "  dutch_text = infile.read()\n",
        "\n",
        "with open(\"/content/exam/hungarian.txt\", 'r') as infile:\n",
        "  hungarian_text = infile.read()\n",
        "\n",
        "with open(\"/content/exam/portugese.txt\", 'r') as infile:\n",
        "  portugese_text = infile.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "class Vocabulary:\n",
        "  def __init__(self, lists):\n",
        "    self.UNKNOWN = \"UNK\"\n",
        "\n",
        "    tokens = []\n",
        "    for sub_list in lists:\n",
        "      tokens.extend(sub_list)\n",
        "      counter = [ item[0] for item in Counter(tokens).most_common(1000)]\n",
        "      self.index2word = [\"UNK\"] + counter\n",
        "      self.word2index = {item : index for index, item in enumerate(self.index2word)}"
      ],
      "metadata": {
        "id": "bS_xb_VrBYyD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(dutch_text, hungarian_text, portugese_text):\n",
        "  dutch_text = dutch_text.split('\\n')\n",
        "  hungarian_text = hungarian_text.split('\\n')\n",
        "  portugese_text = portugese_text.split('\\n')\n",
        "\n",
        "  train_text = {\"dutch\" : dutch_text[:int(len(dutch_text) * 0.8)], \"hungarian\" : hungarian_text[:int(len(hungarian_text) * 0.8)], \"portugese\" : portugese_text[:int(len(portugese_text) * 0.8)]}\n",
        "  test_text = {\"dutch\" : dutch_text[int(len(dutch_text) * 0.8) :], \"hungarian\" : hungarian_text[int(len(hungarian_text) * 0.8) :], \"portugese\" : portugese_text[int(len(portugese_text) * 0.8) :]}\n",
        "\n",
        "  vocab = Vocabulary(train_text.values())\n",
        "\n",
        "  train_tokens = dict()\n",
        "  test_tokens = dict()\n",
        "  for language in train_text.keys():\n",
        "    train_tokens[language] = [vocab.word2index.get(item, 0) for item in train_text[language]]\n",
        "    test_tokens[language] = [vocab.word2index.get(item, 0) for item in test_text[language]]\n",
        "\n",
        "  return train_tokens, test_tokens, vocab"
      ],
      "metadata": {
        "id": "8QJv2z5s_ohe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokens, test_tokens, vocab = split_data(dutch_text, hungarian_text, portugese_text)"
      ],
      "metadata": {
        "id": "2lBT1HcaHbaY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class Net(nn.Module):\n",
        "  def __init__(self, embedding_input: int):\n",
        "    super().__init__()\n",
        "    self.embadding = nn.Embedding(embedding_input, 64)\n",
        "    self.linear_1 = nn.Linear(64, 16)\n",
        "    self.linear_2 = nn.Linear(16, 3)\n",
        "\n",
        "  def forward(self, token):\n",
        "    relu = nn.ReLU()\n",
        "    output = self.embadding(token)\n",
        "    output = relu(output)\n",
        "    output = self.linear_1(output)\n",
        "    output = relu(output)\n",
        "    output = self.linear_2(output)\n",
        "    output = nn.Sigmoid()(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "PD4Xkp6EH8nW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self, tokens_by_languages):\n",
        "    super().__init__()\n",
        "    self.target_codes = {\n",
        "        \"dutch\" : 0,\n",
        "        \"hungarian\": 1,\n",
        "        \"portugese\": 2\n",
        "    }\n",
        "    self.data = []\n",
        "    self.target = []\n",
        "\n",
        "    for lang in tokens_by_languages.keys():\n",
        "      for token in  tokens_by_languages[lang]:\n",
        "        self.data.append(token)\n",
        "        self.target.append(self.target_codes[lang])\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return torch.tensor(self.data[index]), torch.tensor(self.target[index])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.target)"
      ],
      "metadata": {
        "id": "oycAIuCHK8Or"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MyDataset(train_tokens)\n",
        "test_dataset = MyDataset(test_tokens)\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=8)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)"
      ],
      "metadata": {
        "id": "cWC6kurPLrWS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "EPOCHS = 5\n",
        "model = Net(len(vocab.index2word))\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "for e in range(EPOCHS):\n",
        "  train_loss = []\n",
        "  test_loss = []\n",
        "  for batch in train_loader:\n",
        "    optim.zero_grad()\n",
        "    X, Y = batch\n",
        "    output = model(X)\n",
        "    loss = criterion(output, Y)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    train_loss.append(loss.item())\n",
        "  for batch in test_loader:\n",
        "    X, Y = batch\n",
        "    output = model(X)\n",
        "    loss = criterion(output, Y)\n",
        "    test_loss.append(loss.item())\n",
        "\n",
        "  print(\"Epoch\", e, \"Train\", np.mean(train_loss), \"Test\", np.mean(test_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3DH0HoTNQm2",
        "outputId": "1a6dd894-85ac-4f13-a22d-130306048c04"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Train 0.8910402947739923 Test 1.1970766858315804\n",
            "Epoch 1 Train 1.0681929307108369 Test 1.1975187813731987\n",
            "Epoch 2 Train 0.9236955717941602 Test 1.1974039069363769\n",
            "Epoch 3 Train 1.305106542006198 Test 1.2257074576028635\n",
            "Epoch 4 Train 1.2784567605204913 Test 1.2151526239556325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "входной вектор длины (макс. количество символов)\n",
        "1) Embedding - превращаем код символа в вектор - выдает тензор размера (макс. количество символов X эмбеддинг одного символа)\n",
        "2) BI LSTM - распознаем зависимости между символами - выдаёт тензор (макс. количество символов X 2*эмбеддинг одного символа)\n",
        "3) Pooling или сумма вдоль оси - сжимаем матрицу в вектор - выдаём тензор длины (макс. количество символов)\n",
        "4) Linear - превращаем вектор в распределение вероятностей - выдаёт тензор длины 3\n",
        "5) Sigmoid - ограничиваем числа от 0 до 1"
      ],
      "metadata": {
        "id": "AVKjsJdMOhCn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}